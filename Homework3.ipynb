{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "import json\n",
    "import random\n",
    "import gzip\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def readGz(f):\n",
    "  for l in gzip.open(f):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks (Purchase prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.split the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rstudio/Desktop/MSBA/FALL /CSE258 Recommended System and Web Mining/assignment 1/assignment1\n"
     ]
    }
   ],
   "source": [
    "cd assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(readGz(\"train.json.gz\"))\n",
    "data_1 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_1)\n",
    "train = data_1[:100000]\n",
    "validation = data_1[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewTime': '12 24, 2012',\n",
       " 'reviewText': \"As always, very happy with my kenneth cole purchase,the peacoat is high quality, fashionable fit, buttons are solid, the bib easily unzips, which is good because i prefer not to have one and use a scarf if its super cold. Im 6'1 180 lb and ordered a medium, which is slightly big but a small would be too small. The sleeve length is perfect though. The coat works perfect with jeans and casual as it does dress pants and suit jacket.\",\n",
       " 'helpful': {'nHelpful': 4, 'outOf': 5},\n",
       " 'reviewerID': 'U360404137',\n",
       " 'reviewHash': 'R772706618',\n",
       " 'categories': [['Clothing, Shoes & Jewelry', 'K', 'Kenneth Cole New York'],\n",
       "  ['Clothing, Shoes & Jewelry',\n",
       "   'Men',\n",
       "   'Clothing',\n",
       "   'Jackets & Coats',\n",
       "   'Wool & Blends']],\n",
       " 'unixReviewTime': 1356307200,\n",
       " 'itemID': 'I391197657',\n",
       " 'rating': 5.0,\n",
       " 'summary': 'Very nice peacoat!',\n",
       " 'categoryID': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Build such a set by randomly sampling users and items until you have 100,000 non-purchased user/item pairs.Evaluate the performance (accuracy) of the baseline model on the validation set you have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U470297302-I490884580'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## positive sample \n",
    "validation_positive = []\n",
    "for l in validation:\n",
    "        validation_positive.append(l['reviewerID']+'-'+l['itemID'])\n",
    "validation_positive[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pairs existed\n",
    "allReviewerID=[]\n",
    "allItemID=[]\n",
    "\n",
    "for i in data_1:\n",
    "    allReviewerID.append(i['reviewerID'])\n",
    "    allItemID.append(i['itemID'])\n",
    "    \n",
    "set_allR = list(set(allReviewerID))\n",
    "set_allI = list(set(allItemID))\n",
    "\n",
    "purchase_pair = {}\n",
    "for content in data_1:\n",
    "    if purchase_pair.get(content['reviewerID']):\n",
    "        purchase_pair[content['reviewerID']].append(content['itemID'])\n",
    "    else:\n",
    "        purchase_pair[content['reviewerID']] = [content['itemID']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## negative samples \n",
    "count = 0\n",
    "validation_negative = {}\n",
    "while count < len(train):\n",
    "    rid = set_allR[random.randint(0,len(set_allR)-1)]\n",
    "    iid = set_allI[random.randint(0,len(set_allI)-1)]\n",
    "    if iid in purchase_pair[rid]:\n",
    "        count = count\n",
    "    elif iid not in purchase_pair[rid]:\n",
    "        if (validation_negative.get(rid)):\n",
    "            if iid in validation_negative[rid]:\n",
    "                count = count\n",
    "            else:\n",
    "                validation_negative[rid].append(iid)\n",
    "                count += 1\n",
    "        else:\n",
    "            validation_negative[rid]= [iid]\n",
    "            count += 1  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## negative samples to list\n",
    "\n",
    "validation_negative_list = []\n",
    "\n",
    "for i in validation_negative.keys():\n",
    "    if len(validation_negative[i]) > 1:\n",
    "        for ii in validation_negative[i]:\n",
    "            validation_negative_list.append([i,ii])\n",
    "    else:\n",
    "        validation_negative_list.append([i,validation_negative[i][0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "\n",
    "for l in train:\n",
    "    user,item = l['reviewerID'],l['itemID']\n",
    "    itemCount[item] += 1\n",
    "    totalPurchases += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostPopular = [(itemCount[x], x) for x in itemCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPurchases/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for the baseline code is 0.630145\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for l in validation_positive:\n",
    "    u,i = l.strip().split('-')\n",
    "    if i in return1:\n",
    "        count += 1\n",
    "\n",
    "for r, i in validation_negative_list:\n",
    "    if not i in return1:\n",
    "        count = count +1\n",
    "        \n",
    "print('accuracy for the baseline code is ' + str(count / 200000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.The existing ‘purchase prediction’ baseline just returns True if the item in question is ‘popular,’ using a threshold of the 50th percentile of popularity (totalPurchases/2). Assuming that the ‘non-purchased’ test examples are a random sample of user-purchase pairs, is this particular threshold value the best? If not, see if you can find a better one (and report its performance), or if so, explain why it is the best (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {}\n",
    "for n in numpy.arange(0, 100, 1):     \n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalPurchases * (n/100) : break\n",
    "    count_1 = 0 \n",
    "    for l in validation_positive:\n",
    "        u,i = l.strip().split('-')\n",
    "        if i in return1:\n",
    "            count_1 += 1\n",
    "    for r, i in validation_negative_list:\n",
    "        if not i in return1:\n",
    "            count_1 = count_1 +1\n",
    "    accuracy[n] = count_1 / 200000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accuracy,key=accuracy.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the threshold is 54%, the accracy is max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Users may tend to repeatedly purchase items of the same type. Build a baseline that returns ‘True’ if a user has purchased an item of the same category before (at least one category in common), or zero otherwise (1 mark).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pairs existed\n",
    "\n",
    "trainReviewerID=[]\n",
    "trainCategory={}\n",
    "\n",
    "for i in data_1:\n",
    "    trainReviewerID.append(i['reviewerID'])\n",
    "    \n",
    "set_trainR = list(set(trainReviewerID))\n",
    "\n",
    "for content in data_1:\n",
    "    if trainCategory.get(content['reviewerID']):\n",
    "        for u in content['categories']:\n",
    "            trainCategory[content['reviewerID']].append(u)\n",
    "    else:\n",
    "        temp = []\n",
    "        for u in content['categories']:\n",
    "            temp.append(u)\n",
    "            trainCategory[content['reviewerID']] = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories = defaultdict(list)\n",
    "for i in data_1:\n",
    "    item = i['itemID']\n",
    "    categories = i['categories']\n",
    "    item_categories[item] = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2 = 0 \n",
    "for l in validation_positive:\n",
    "    u,i = l.strip().split('-')\n",
    "    if trainCategory.get(u):\n",
    "        if sum([c in trainCategory[u] for c in item_categories[i]]) > 0 :\n",
    "            count_2 += 1\n",
    "        else: count_2 = count_2\n",
    "        \n",
    "for u,i in validation_negative_list:\n",
    "    if trainCategory.get(u):\n",
    "        if sum([c in trainCategory[u] for c in item_categories[i]]) == 0 :\n",
    "            count_2 += 1 \n",
    "        else: count_2 = count_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for the baseline code is 0.558805\n"
     ]
    }
   ],
   "source": [
    "print('accuracy for the baseline code is ' + str(count_2 / 200000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.To run our model on the test set, we’ll have to use the files ‘pairs Purchase.txt’ to find the review- erID/itemID pairs about which we have to make predictions. Using that data, run the above model and upload your solution to Kaggle. Tell us your Kaggle user name (1 mark). If you’ve already uploaded a better solution to Kaggle, that’s fine too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Purchase.txt\", 'w')\n",
    "for l in open(\"pairs_Purchase.txt\"):\n",
    "    if l.startswith(\"reviewerID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    if trainCategory.get(u):\n",
    "        if sum([c in trainCategory[u] for c in item_categories[i]]) > 0 :\n",
    "            predictions.write(u + '-' + i + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + '-' + i + \",0\\n\")\n",
    "    else: predictions.write(u + '-' + i + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My id in kaggle is ‘Lu Yao’, and the accuracy is 0.59971."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks (Rating prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.What is the performance of a trivial predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "\n",
    "for l in train:\n",
    "    allRatings.append(l['rating'])\n",
    "\n",
    "globalAverage = sum(allRatings) / len(allRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.23691"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "rid = []\n",
    "iid = []\n",
    "rating = []\n",
    "predict = []\n",
    "for l in validation:\n",
    "    u,i,r = l['reviewerID'],l['itemID'],l['rating']\n",
    "    rid.append(u)\n",
    "    iid.append(i)\n",
    "    rating.append(r)\n",
    "    predict.append(globalAverage)\n",
    "\n",
    "    \n",
    "predictions  = set(zip(rid,iid,rating,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2284284599000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(rating, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Fit a predictor of the form rating(user, item) ≃ α + βuser + βitem, by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization parameter of λ = 1. Report the MSE on the validation set (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate initial value of $\\beta_{user}$ and $\\beta_{item}$\n",
    "\n",
    "userRatings = defaultdict(list)\n",
    "itemRatings = defaultdict(list)\n",
    "index = 0\n",
    "alpha = globalAverage\n",
    "\n",
    "for l in data_1:\n",
    "    user,item = l['reviewerID'],l['itemID']\n",
    "    userRatings[user].append(l[\"rating\"])\n",
    "    itemRatings[item].append(l[\"rating\"])\n",
    "\n",
    "userAverage = defaultdict(float)\n",
    "for u in userRatings:\n",
    "    userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n",
    "itemAverage = defaultdict(float)\n",
    "for b in itemRatings:\n",
    "    itemAverage[b] = sum(itemRatings[b]) / len(itemRatings[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initial value of $\\alpha$ is globalAverage, initial value of $\\beta_{user}$ is userAverage, initial value of $\\beta_{item}$ is itemAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converge(value, value_last, epsilon):\n",
    "    error = 0\n",
    "    for i in value:\n",
    "        error += abs(value[i] - value_last[i])\n",
    "    #print(error)\n",
    "    if error < epsilon:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itemAverage_last' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e596a7966431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewerID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'itemID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnum_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0muserAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mglobalAverage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mitemAverage_last\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserAverage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0muserAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itemAverage_last' is not defined"
     ]
    }
   ],
   "source": [
    "        # update userAverage\n",
    "        num_item = defaultdict(int)\n",
    "        for user in userAverage:\n",
    "            userAverage[user] = 0.0\n",
    "        for l in data:\n",
    "            user,item = l['reviewerID'],l['itemID']\n",
    "            num_item[user] += 1\n",
    "            userAverage[user] += (l[\"rating\"] - globalAverage - itemAverage_last[item])\n",
    "        for user in userAverage:\n",
    "            userAverage[user] /= (lam + num_item[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(globalAverage, userAverage, itemAverage, lam = 1, epsilon = 1e-5):\n",
    "    finish = False\n",
    "    globalAverage_last = globalAverage\n",
    "    userAverage_last = userAverage\n",
    "    itemAverage_last = itemAverage\n",
    "    while not finish:\n",
    "        # update globalAverage\n",
    "        num_train = len(data)\n",
    "        globalAverage = 0.0\n",
    "        for l in data_1:\n",
    "            user,item = l['reviewerID'],l['itemID']\n",
    "            globalAverage += (l[\"rating\"] - userAverage_last[user] - \n",
    "                              itemAverage_last[item])\n",
    "        globalAverage /= num_train\n",
    "        \n",
    "        # update userAverage\n",
    "        num_item = defaultdict(int)\n",
    "        for user in userAverage:\n",
    "            userAverage[user] = 0.0\n",
    "        for l in data_1:\n",
    "            user,item = l['reviewerID'],l['itemID']\n",
    "            num_item[user] += 1\n",
    "            userAverage[user] += (l[\"rating\"] - globalAverage - itemAverage_last[item])\n",
    "        for user in userAverage:\n",
    "            userAverage[user] /= (lam + num_item[user])\n",
    "        \n",
    "        # update itemAverage\n",
    "        num_user = defaultdict(int)\n",
    "        for item in itemAverage:\n",
    "            itemAverage[item] = 0.0\n",
    "        for l in data_1:\n",
    "            user,item = l['reviewerID'],l['itemID']\n",
    "            num_user[item] += 1\n",
    "            itemAverage[item] += (l[\"rating\"] - globalAverage - userAverage[user])\n",
    "        for item in itemAverage:\n",
    "            itemAverage[item] /= (lam + num_user[item])\n",
    "        \n",
    "        #print ((globalAverage - globalAverage_last)**2)\n",
    "        if abs(globalAverage - globalAverage_last) < epsilon and converge(userAverage, userAverage_last, epsilon) and converge(itemAverage, itemAverage_last, epsilon):\n",
    "            finish = True\n",
    "        else:\n",
    "            globalAverage_last = globalAverage\n",
    "            userAverage_last = userAverage\n",
    "            itemAverage_last = itemAverage\n",
    "    return globalAverage, userAverage, itemAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAverage_new, userAverage_new, itemAverage_new = update(globalAverage, userAverage, itemAverage, lam = 1, epsilon = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mse(globalAverage_new, userAverage_new, itemAverage_new):\n",
    "    mse = 0.0\n",
    "    for l in data_1:\n",
    "        user,business = l['reviewerID'],l['itemID']\n",
    "        mse += (l[\"rating\"]-globalAverage_new-userAverage_new[user]-\\\n",
    "                itemAverage_new[business])**2\n",
    "    mse /= 100000\n",
    "    print (\"mean square error on validation data is \" + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean square error on validation data is 0.6839153332902849\n"
     ]
    }
   ],
   "source": [
    "cal_mse(globalAverage_new, userAverage_new, itemAverage_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Report the user and item IDs that have the largest and smallest values of β (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "userAverage_new_sort = sorted(userAverage_new.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('U933497871', -2.5253677137173898)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAverage_new_sort[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('U605818049', 1.4494614247226256)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userAverage_new_sort[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemAverage_new_sort = sorted(itemAverage_new.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I119256103', -2.399751075797392)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemAverage_new_sort[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I043564971', 1.3742619052625877)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemAverage_new_sort[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id with maximum beta is ('U605818049', 1.4494614247226256)\n",
      "user id with minimum beta is ('U933497871', -2.5253677137173898)\n",
      "business id with maximum beta is ('I043564971', 1.3742619052625877)\n",
      "business id with minimum beta is ('I119256103', -2.399751075797392)\n"
     ]
    }
   ],
   "source": [
    "print (\"user id with maximum beta is\",userAverage_new_sort[-1])\n",
    "\n",
    "print (\"user id with minimum beta is\",userAverage_new_sort[0])\n",
    "\n",
    "print (\"business id with maximum beta is\",itemAverage_new_sort[-1])\n",
    "\n",
    "print (\"business id with minimum beta is\",itemAverage_new_sort[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data (1 mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c6544cb01d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mglobalAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemAverage_new1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobalAverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserAverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemAverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"lambda is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcal_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobalAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemAverage_new1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4792d0f38732>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(globalAverage, userAverage, itemAverage, lam, epsilon)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0muserAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mglobalAverage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mitemAverage_last\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserAverage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0muserAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# update itemAverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#lams = numpy.linspace(0,1,10)\n",
    "lams = [0.01, 0.1, 10, 100]\n",
    "for lam in lams:\n",
    "    globalAverage_new1, userAverage_new1, itemAverage_new1 = update(globalAverage, userAverage, itemAverage, lam = lam, epsilon = 1e-6)\n",
    "    print (\"lambda is \" + str(lam))\n",
    "    cal_mse(globalAverage_new1, userAverage_new1, itemAverage_new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 1\n",
      "mean square error on validation data is 1.364446495110649\n",
      "lambda is 2\n",
      "mean square error on validation data is 1.4270198088388026\n",
      "lambda is 3\n",
      "mean square error on validation data is 1.4873165137070894\n",
      "lambda is 4\n",
      "mean square error on validation data is 1.5419956240818535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fc689a81582a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mglobalAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemAverage_new1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobalAverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserAverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemAverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"lambda is \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcal_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobalAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserAverage_new1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemAverage_new1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4792d0f38732>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(globalAverage, userAverage, itemAverage, lam, epsilon)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mitemAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewerID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'itemID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mnum_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mitemAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mglobalAverage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muserAverage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lams = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "    \n",
    "for lam in lams:\n",
    "    globalAverage_new1, userAverage_new1, itemAverage_new1 = update(globalAverage, userAverage, itemAverage, lam = lam, epsilon = 1e-6)\n",
    "    print (\"lambda is \" + str(lam))\n",
    "    cal_mse(globalAverage_new1, userAverage_new1, itemAverage_new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"revierweID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "        \n",
    "    user,item = l.strip().split('-')\n",
    "    predictions.write(user + '-' + item + ',' + \\\n",
    "                str(globalAverage_new1 + userAverage_new1[user]\\\n",
    "                    +itemAverage_new1[item]) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
